{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='resnet50', base_lr=0.1, batch_size=1024, cov_coeff=1.0, device='cuda', dist_url='env://', epochs=100, exp_dir=PosixPath('exp'), local_rank=-1, log_freq_time=60, mlp='1024-1024-512', num_workers=4, sim_coeff=25.0, std_coeff=25.0, wd=0.0001, world_size=1)\n",
      "main_vicreg.py --batch-size 1024 --mlp 1024-1024-512\n",
      "Files already downloaded and verified\n",
      "{\"epoch\": 1, \"step\": 63, \"loss\": 20.535282135009766, \"time\": 60, \"lr\": 0.051428571428571435}\n",
      "{\"epoch\": 2, \"step\": 132, \"loss\": 19.910831451416016, \"time\": 120, \"lr\": 0.10775510204081634}\n",
      "{\"epoch\": 4, \"step\": 198, \"loss\": 19.32766342163086, \"time\": 180, \"lr\": 0.16163265306122448}\n",
      "{\"epoch\": 5, \"step\": 267, \"loss\": 18.44390869140625, \"time\": 241, \"lr\": 0.21795918367346942}\n",
      "{\"epoch\": 6, \"step\": 335, \"loss\": 17.307598114013672, \"time\": 301, \"lr\": 0.27346938775510204}\n",
      "{\"epoch\": 8, \"step\": 402, \"loss\": 16.366077423095703, \"time\": 361, \"lr\": 0.3281632653061225}\n",
      "{\"epoch\": 9, \"step\": 471, \"loss\": 15.74398136138916, \"time\": 422, \"lr\": 0.38448979591836735}\n",
      "{\"epoch\": 11, \"step\": 539, \"loss\": 14.694072723388672, \"time\": 483, \"lr\": 0.3998782872384154}\n",
      "{\"epoch\": 12, \"step\": 606, \"loss\": 13.804800033569336, \"time\": 544, \"lr\": 0.39931820018369535}\n",
      "{\"epoch\": 13, \"step\": 674, \"loss\": 13.157899856567383, \"time\": 604, \"lr\": 0.39828603560243836}\n",
      "{\"epoch\": 15, \"step\": 740, \"loss\": 12.347248077392578, \"time\": 664, \"lr\": 0.3968397614663032}\n",
      "{\"epoch\": 16, \"step\": 810, \"loss\": 11.882760047912598, \"time\": 725, \"lr\": 0.3948310007407318}\n",
      "{\"epoch\": 17, \"step\": 879, \"loss\": 11.130468368530273, \"time\": 785, \"lr\": 0.39237734534616037}\n",
      "{\"epoch\": 19, \"step\": 945, \"loss\": 10.784104347229004, \"time\": 846, \"lr\": 0.38959588376419296}\n",
      "{\"epoch\": 20, \"step\": 1014, \"loss\": 10.1094388961792, \"time\": 906, \"lr\": 0.3862405280049596}\n",
      "{\"epoch\": 22, \"step\": 1080, \"loss\": 9.94565200805664, \"time\": 967, \"lr\": 0.3826104128141319}\n",
      "{\"epoch\": 23, \"step\": 1148, \"loss\": 9.559093475341797, \"time\": 1027, \"lr\": 0.37844871295268395}\n",
      "{\"epoch\": 24, \"step\": 1219, \"loss\": 9.453699111938477, \"time\": 1087, \"lr\": 0.3736572939255853}\n",
      "{\"epoch\": 26, \"step\": 1285, \"loss\": 8.802238464355469, \"time\": 1148, \"lr\": 0.3688051386390651}\n",
      "{\"epoch\": 27, \"step\": 1353, \"loss\": 8.506322860717773, \"time\": 1208, \"lr\": 0.36341630843857076}\n",
      "{\"epoch\": 29, \"step\": 1421, \"loss\": 8.6101655960083, \"time\": 1271, \"lr\": 0.35764454857062306}\n",
      "{\"epoch\": 30, \"step\": 1488, \"loss\": 7.953420639038086, \"time\": 1331, \"lr\": 0.3515963195421303}\n",
      "{\"epoch\": 31, \"step\": 1556, \"loss\": 7.934045314788818, \"time\": 1391, \"lr\": 0.3451053030713614}\n",
      "{\"epoch\": 33, \"step\": 1622, \"loss\": 7.625925064086914, \"time\": 1451, \"lr\": 0.33847993273102944}\n",
      "{\"epoch\": 34, \"step\": 1688, \"loss\": 7.782083511352539, \"time\": 1512, \"lr\": 0.33154893688762876}\n",
      "{\"epoch\": 35, \"step\": 1757, \"loss\": 7.485417366027832, \"time\": 1572, \"lr\": 0.3239927517478834}\n",
      "{\"epoch\": 37, \"step\": 1818, \"loss\": 7.500823974609375, \"time\": 1633, \"lr\": 0.31706299902030277}\n",
      "{\"epoch\": 38, \"step\": 1881, \"loss\": 7.242098331451416, \"time\": 1693, \"lr\": 0.30967458175449586}\n",
      "{\"epoch\": 39, \"step\": 1948, \"loss\": 7.1055450439453125, \"time\": 1753, \"lr\": 0.3015755036635323}\n",
      "{\"epoch\": 41, \"step\": 2011, \"loss\": 7.14237117767334, \"time\": 1813, \"lr\": 0.29374897729213717}\n",
      "{\"epoch\": 41, \"step\": 2056, \"loss\": 7.128337860107422, \"time\": 1874, \"lr\": 0.2880423229359873}\n",
      "{\"epoch\": 43, \"step\": 2112, \"loss\": 7.1964335441589355, \"time\": 1934, \"lr\": 0.2808153270762891}\n",
      "{\"epoch\": 44, \"step\": 2181, \"loss\": 6.9311065673828125, \"time\": 1994, \"lr\": 0.2717354719489717}\n",
      "{\"epoch\": 45, \"step\": 2251, \"loss\": 6.503992080688477, \"time\": 2055, \"lr\": 0.26234755543877947}\n",
      "{\"epoch\": 47, \"step\": 2316, \"loss\": 6.397934913635254, \"time\": 2115, \"lr\": 0.25349135233872105}\n",
      "{\"epoch\": 48, \"step\": 2386, \"loss\": 6.426005840301514, \"time\": 2176, \"lr\": 0.24382669029519166}\n",
      "{\"epoch\": 50, \"step\": 2453, \"loss\": 6.237984657287598, \"time\": 2237, \"lr\": 0.23447431393326806}\n",
      "{\"epoch\": 51, \"step\": 2521, \"loss\": 6.404254913330078, \"time\": 2297, \"lr\": 0.22490263713509515}\n",
      "{\"epoch\": 52, \"step\": 2590, \"loss\": 6.103851318359375, \"time\": 2358, \"lr\": 0.2151310726985676}\n",
      "{\"epoch\": 54, \"step\": 2652, \"loss\": 6.202820301055908, \"time\": 2419, \"lr\": 0.20631937828958985}\n",
      "{\"epoch\": 55, \"step\": 2712, \"loss\": 6.058701992034912, \"time\": 2480, \"lr\": 0.19778039166087188}\n",
      "{\"epoch\": 56, \"step\": 2769, \"loss\": 6.210204124450684, \"time\": 2541, \"lr\": 0.18967220744824256}\n",
      "{\"epoch\": 57, \"step\": 2826, \"loss\": 6.081034183502197, \"time\": 2601, \"lr\": 0.18158137924686413}\n",
      "{\"epoch\": 59, \"step\": 2893, \"loss\": 5.924897193908691, \"time\": 2661, \"lr\": 0.17211134634102881}\n",
      "{\"epoch\": 60, \"step\": 2964, \"loss\": 6.069479465484619, \"time\": 2722, \"lr\": 0.1621462242988532}\n",
      "{\"epoch\": 61, \"step\": 3034, \"loss\": 5.7339186668396, \"time\": 2783, \"lr\": 0.1524166226364167}\n",
      "{\"epoch\": 63, \"step\": 3100, \"loss\": 5.851373195648193, \"time\": 2843, \"lr\": 0.14335138819111973}\n",
      "{\"epoch\": 64, \"step\": 3170, \"loss\": 5.860409736633301, \"time\": 2904, \"lr\": 0.13387448298602062}\n",
      "{\"epoch\": 66, \"step\": 3236, \"loss\": 5.752399444580078, \"time\": 2964, \"lr\": 0.12508974026503006}\n",
      "{\"epoch\": 67, \"step\": 3306, \"loss\": 5.709301948547363, \"time\": 3024, \"lr\": 0.11595441625329156}\n",
      "{\"epoch\": 68, \"step\": 3376, \"loss\": 5.8609395027160645, \"time\": 3084, \"lr\": 0.10702854003666304}\n",
      "{\"epoch\": 70, \"step\": 3442, \"loss\": 5.567709922790527, \"time\": 3145, \"lr\": 0.09882449633646777}\n",
      "{\"epoch\": 71, \"step\": 3511, \"loss\": 5.49314022064209, \"time\": 3206, \"lr\": 0.09048739709707888}\n",
      "{\"epoch\": 73, \"step\": 3578, \"loss\": 5.5578413009643555, \"time\": 3266, \"lr\": 0.08264538627659}\n",
      "{\"epoch\": 74, \"step\": 3648, \"loss\": 5.4265031814575195, \"time\": 3326, \"lr\": 0.07473851966019775}\n",
      "{\"epoch\": 75, \"step\": 3719, \"loss\": 5.533304691314697, \"time\": 3387, \"lr\": 0.06703742339404928}\n",
      "{\"epoch\": 77, \"step\": 3785, \"loss\": 5.479494571685791, \"time\": 3447, \"lr\": 0.06018371105072201}\n",
      "{\"epoch\": 78, \"step\": 3855, \"loss\": 5.430643558502197, \"time\": 3508, \"lr\": 0.053253109702947936}\n",
      "{\"epoch\": 80, \"step\": 3920, \"loss\": 5.406647682189941, \"time\": 3569, \"lr\": 0.04714432026482822}\n",
      "{\"epoch\": 81, \"step\": 3990, \"loss\": 5.56154727935791, \"time\": 3629, \"lr\": 0.04093292505686009}\n",
      "{\"epoch\": 82, \"step\": 4058, \"loss\": 5.4604949951171875, \"time\": 3690, \"lr\": 0.03527801888833063}\n",
      "{\"epoch\": 84, \"step\": 4124, \"loss\": 5.6488118171691895, \"time\": 3750, \"lr\": 0.030159343660969288}\n",
      "{\"epoch\": 85, \"step\": 4193, \"loss\": 5.542522430419922, \"time\": 3811, \"lr\": 0.025210009868915063}\n",
      "{\"epoch\": 87, \"step\": 4263, \"loss\": 5.66028356552124, \"time\": 3874, \"lr\": 0.020620949549426427}\n",
      "{\"epoch\": 88, \"step\": 4332, \"loss\": 5.296213626861572, \"time\": 3935, \"lr\": 0.016534338239157297}\n",
      "{\"epoch\": 89, \"step\": 4402, \"loss\": 5.273993492126465, \"time\": 3995, \"lr\": 0.01284190829364084}\n",
      "{\"epoch\": 91, \"step\": 4468, \"loss\": 5.247647285461426, \"time\": 4056, \"lr\": 0.009786965364016844}\n",
      "{\"epoch\": 92, \"step\": 4537, \"loss\": 5.262109756469727, \"time\": 4116, \"lr\": 0.00704323818254549}\n",
      "{\"epoch\": 94, \"step\": 4606, \"loss\": 5.519366264343262, \"time\": 4179, \"lr\": 0.004766109373385646}\n",
      "{\"epoch\": 95, \"step\": 4675, \"loss\": 5.360099792480469, \"time\": 4240, \"lr\": 0.002961079673792855}\n",
      "{\"epoch\": 96, \"step\": 4745, \"loss\": 5.314189910888672, \"time\": 4300, \"lr\": 0.0016167746462182777}\n",
      "{\"epoch\": 98, \"step\": 4811, \"loss\": 5.115177154541016, \"time\": 4360, \"lr\": 0.0008014417917045426}\n",
      "{\"epoch\": 99, \"step\": 4880, \"loss\": 5.307594299316406, \"time\": 4420, \"lr\": 0.0004202787275037602}\n"
     ]
    }
   ],
   "source": [
    "!python main_vicreg.py --batch-size 1024  --mlp '1024-1024-512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load the SSL model and its backbone\n",
    "ssl_model_state_dict = torch.load(\"exp/resnet_backbone.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset with 1% labeled data\n",
    "trainset = datasets.CIFAR10(root=\"../data\", train=True, download=True, transform=transform)\n",
    "# trainset_1percent = torch.utils.data.Subset(trainset, range(0, 500))  # Only use 1% of the labels (i.e., 500 out of 50,000)\n",
    "\n",
    "# # Load the CIFAR-10 dataset with 10% labeled data\n",
    "# trainset_10percent = torch.utils.data.Subset(trainset, range(0, 5000)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the percentage of labels to take from each class\n",
    "percent_per_class = 0.1\n",
    "\n",
    "# Get the list of classes\n",
    "classes = trainset.classes\n",
    "\n",
    "# Create a dictionary to hold the indices of the images for each class\n",
    "indices = {}\n",
    "for c in classes:\n",
    "    indices[c] = []\n",
    "\n",
    "# Populate the dictionary with the indices of the images for each class\n",
    "for i in range(len(trainset)):\n",
    "    _, label = trainset[i]\n",
    "    indices[classes[label]].append(i)\n",
    "\n",
    "# Create a list of indices to use for each class based on the percentage\n",
    "subset_indices = []\n",
    "for c in classes:\n",
    "    num_images = len(indices[c])\n",
    "    num_subset_images = int(num_images * percent_per_class)\n",
    "    subset_indices.extend(indices[c][:num_subset_images])\n",
    "\n",
    "# Create a subset sampler using the subset indices\n",
    "subset_sampler = torch.utils.data.sampler.SubsetRandomSampler(subset_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = datasets.CIFAR10(root='../data', train=False,\n",
    "                                       download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader using the subset sampler\n",
    "cifar10_1pct_train_loader = torch.utils.data.DataLoader(trainset, batch_size=256, sampler=subset_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader using the subset sampler\n",
    "cifar10_10pct_train_loader = torch.utils.data.DataLoader(trainset, batch_size=256, sampler=subset_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from custom_resnet import custom_Resnet,Resnet_block\n",
    "head = nn.Sequential(\n",
    "    nn.Linear(in_features=128, out_features=64),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64, out_features=32),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=32, out_features=10)\n",
    ")\n",
    "\n",
    "backbone= custom_Resnet(Resnet_block,32,[13,13,13])\n",
    "backbone.load_state_dict(ssl_model_state_dict)\n",
    "# Combine the backbone and the head\n",
    "\n",
    "model = nn.Sequential(\n",
    "    backbone,\n",
    "    head\n",
    ")\n",
    "\n",
    "# Freeze the backbone weights\n",
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(head.parameters(), lr=1e-3)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.442\n",
      "[2,    20] loss: 0.410\n",
      "[3,    20] loss: 0.393\n",
      "[4,    20] loss: 0.382\n",
      "[5,    20] loss: 0.374\n",
      "[6,    20] loss: 0.367\n",
      "[7,    20] loss: 0.364\n",
      "[8,    20] loss: 0.360\n",
      "[9,    20] loss: 0.357\n",
      "[10,    20] loss: 0.355\n",
      "[11,    20] loss: 0.353\n",
      "[12,    20] loss: 0.354\n",
      "[13,    20] loss: 0.352\n",
      "[14,    20] loss: 0.349\n",
      "[15,    20] loss: 0.350\n",
      "[16,    20] loss: 0.348\n",
      "[17,    20] loss: 0.347\n",
      "[18,    20] loss: 0.348\n",
      "[19,    20] loss: 0.343\n",
      "[20,    20] loss: 0.346\n"
     ]
    }
   ],
   "source": [
    "num_epochs=20\n",
    "device =torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(cifar10_10pct_train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "   \n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / len(cifar10_10pct_train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34679766654968264\n"
     ]
    }
   ],
   "source": [
    "print(running_loss/100)#10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038683040142059325\n"
     ]
    }
   ],
   "source": [
    "print(running_loss/100)#1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_top_5(model, test_loader,device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.topk(k=5, dim=1)\n",
    "            labels = labels.view(-1, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total * 100\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.65"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_top_5(model,testloader,device)#1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.83"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_top_5(model,testloader,device)#10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to track accuracy and loss\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Disable gradient computation (speeds up inference)\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches in the dataloader\n",
    "        for images, labels in dataloader:\n",
    "            # Move data to the specified device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass to get logits\n",
    "            logits = model(images)\n",
    "\n",
    "            # Compute cross-entropy loss\n",
    "            loss = F.cross_entropy(logits, labels, reduction='sum')\n",
    "\n",
    "            # Compute predictions and accuracy\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            total_correct += torch.sum(predictions == labels)\n",
    "            total_loss += loss.item()\n",
    "            total_samples += images.shape[0]\n",
    "\n",
    "    # Compute average accuracy and loss\n",
    "    avg_accuracy = total_correct / total_samples\n",
    "    avg_loss = total_loss / total_samples\n",
    "\n",
    "    return avg_accuracy.item(), avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.39989998936653137, 1.8280329650878906)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,testloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4715999960899353, 1.5065254426956176)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,testloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
